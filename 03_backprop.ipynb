{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop from scratch\n",
    "In this notebook, we will be implementing backpropagation from scratch.\n",
    "\n",
    "Backprop is one of the well-known ingredients of why neural networks work as well as they do. At its crux, backprop is simply chain rule of derivatives.\n",
    "\n",
    "Let's dive right in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version\n",
    "### Basic architecture\n",
    "Let's start by setting up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1 # number of classes\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m, nh)\n",
    "b1 = torch.randn(nh)\n",
    "# for the output, we set only 1 value as we will be using MSE \n",
    "# and not cross entropy yet\n",
    "# so think of it as trying to predict an integer from 0 to c-1\n",
    "w2 = torch.randn(nh, 1)\n",
    "b2 = torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b):\n",
    "    return x@w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our linear function is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 50])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(x_train, w1, b1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11.65, 11.50,  0.00,  ...,  6.04,  1.36,  9.80],\n",
       "        [27.46,  7.34,  5.09,  ...,  0.00,  0.00, 15.98],\n",
       "        [ 4.80,  0.00,  1.47,  ...,  0.00,  5.63,  3.14],\n",
       "        ...,\n",
       "        [ 0.26,  3.48,  0.00,  ...,  1.26,  5.81, 17.91],\n",
       "        [ 0.00,  5.41,  0.00,  ...,  2.79,  0.07, 10.75],\n",
       "        [ 7.63, 15.03,  0.00,  ..., 14.50,  2.60, 16.91]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a simple model with 1 non-linear hidden layer, followed by the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    l1 = lin(X, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(x_train)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "As mentioned earlier, we will use MSE as our loss function. This does not make much sense (since we are doing categorical classification and not regression), but we are doing this to simplify the initial backprop code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1]), torch.Size([50000]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_valid = y_train.float(), y_valid.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, actual):\n",
    "    return ((pred - actual) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1966.65)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds.squeeze(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = inp.t() @ out.g\n",
    "    b.g = torch.sum(out.g, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    # derivative of loss wrt to lout1 = d(loss)/d(rout1) * d(rout1)/d(lout1)\n",
    "    inp.g = torch.where(inp <= 0, torch.tensor(0.), torch.tensor(1.)) * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(input, target):\n",
    "    # forward pass\n",
    "    lout1 = lin(input, w1, b1)\n",
    "    rout1 = relu(lout1)\n",
    "    lout2 = lin(rout1, w2, b2)\n",
    "    diff = lout2.squeeze() - target\n",
    "    loss = (diff ** 2).mean()\n",
    "\n",
    "    # backward pass\n",
    "    lout2.g = (2 * diff[:, None] / input.shape[0])\n",
    "    # get gradients for rout1, w2, b2\n",
    "    lin_grad(rout1, lout2, w2, b2)\n",
    "    # get gradients for lout1\n",
    "    relu_grad(lout1, rout1)\n",
    "    # get gradients for input, w1, b1\n",
    "    lin_grad(input, lout1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing against later\n",
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PyTorch's `autograd` to check our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, tgt):\n",
    "    l1 = lin(inp, w12, b12)\n",
    "    r1 = relu(l1)\n",
    "    out = lin(r1, w22, b22)\n",
    "    return mse(out.squeeze(), tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 50]) torch.Size([784, 50])\n",
      "torch.Size([50, 1]) torch.Size([50, 1])\n",
      "torch.Size([50]) torch.Size([50])\n",
      "torch.Size([1]) torch.Size([1])\n",
      "torch.Size([50000, 784]) torch.Size([50000, 784])\n"
     ]
    }
   ],
   "source": [
    "for a,b in zip(grads, ptgrads): \n",
    "    print(a.shape, b.shape)\n",
    "    test_close(a, b.grad, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementaton is working well! Now we want to refactor the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu(Module):\n",
    "    def forward(self, inp): \n",
    "        return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): \n",
    "        inp.g = torch.where(inp <= 0, torch.tensor(0.), torch.tensor(1.)) * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, w, b):\n",
    "        self.w, self.b = w, b\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = torch.sum(out.g, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Module):\n",
    "    def forward(self, inp, tgt):\n",
    "        self.inp, self.tgt = inp, tgt\n",
    "        self.diff = inp.squeeze() - tgt\n",
    "        return (self.diff ** 2).mean()\n",
    "    \n",
    "    def bwd(self, out, inp, tgt):\n",
    "        self.inp.g = (2. * self.diff[:, None]) / self.tgt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Linear(w1, b1), ReLu(), Linear(w2, b2)]\n",
    "        self.loss = MSE()\n",
    "\n",
    "    def __call__(self, x, tgt):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, tgt)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)\n",
    "loss = model(x_train, y_train)\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "Now that we have implemented our own version of `Module`, we can use PyTorch's version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in, n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in, nh), nn.ReLU(), Linear(nh, n_out)]\n",
    "\n",
    "    def __call__(self, x, tgt):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return F.mse_loss(x, tgt[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-14.69,   7.04, -16.46,  39.75,  96.35,  18.34,  64.73, -52.65,  77.14,   6.30,  28.86, -25.50,  -0.42,   2.91,\n",
       "         32.97,  -3.70,   5.94,   6.37, -39.08,  -2.62, -42.50,  16.05, -15.62,  39.80,   5.66, -22.96,  -0.89, -39.27,\n",
       "          9.37, -12.24,  14.09,  50.00,  -0.96,  -1.72,   5.20,  14.88,   4.63,   4.50,   1.00,  -7.72, -19.31, -23.15,\n",
       "         -2.87,  -5.87,  37.45, 146.43,  24.27, -82.26,  -5.01,  -2.53])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "np",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
